package lbj;

import java.util.*;
import LbjTagger.NEWord;
import LbjTagger.BrownClusters;
import LbjTagger.Parameters;
import LbjTagger.Gazzetteers;
import StringStatisticsUtils.*;


//---------------- CLASSIFIER LEVEL 1 -------------------

discrete% GazetteersFeatures(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("GazetteersFeatures"))
	{ 
 		int i=0;
   		NEWord w = word, last = (NEWord)word.next;
  
   		for (i = 0; i < 2 && last != null; ++i) last = (NEWord) last.next;
   		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;
  
 		do 
   		{
	 		if(w.gazetteers!=null)
		 		for(int j=0;j<w.gazetteers.size();j++)
					sense i: w.gazetteers.elementAt(j);
	 		i++;
	 		w = (NEWord) w.next;
   		}while(w != last);
 	}
}

// Problem 1
discrete% Forms(NEWord word) <-
{
  	if(Parameters.featuresToUse.containsKey("Forms"))
	{ 
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next) sense i++ : w.form;
  		for (; w != last; w = (NEWord) w.next){
			sense i : MyString.normalizeDigitsForFeatureExtraction(w.form);
			i++;
		}
	}
}


// Problem 1
discrete% BrownClusterPaths(NEWord word) <-
{
  	if(Parameters.featuresToUse.containsKey("BrownClusterPaths"))
	{ 
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next){
  			String[] paths=BrownClusters.getPrefixes(w.form);
  			for(int j=0;j<paths.length;j++)
  				sense i : paths[j];
  			i++;
  		}
	}
}


// Problem 1
discrete% FormParts(NEWord word) <-
{
  	if((!Parameters.tokenizationScheme.equalsIgnoreCase(Parameters.DualTokenizationScheme))
  		&&(!Parameters.tokenizationScheme.equalsIgnoreCase(Parameters.LbjTokenizationScheme)))
  		{
  			System.out.println("Fatal error at FormParts feature extractor: unknown tokenization scheme: " +Parameters.LbjTokenizationScheme);
  			System.exit(0);
  		}
  	if(Parameters.featuresToUse.containsKey("Forms")&&
  		Parameters.tokenizationScheme.equalsIgnoreCase(Parameters.DualTokenizationScheme))
	{ 
		sense "0" : word.form;
		int i=-1;
		int count=-1;
		NEWord w = (NEWord)word.previous;
		while(w!=null&&i>=-2){
			String[] lastParts= w.parts;
			for(int j=0;j<lastParts.length;j++)
			{
				sense count:  MyString.normalizeDigitsForFeatureExtraction(lastParts[j]);
				count--;
			}
			w = (NEWord)w.previous;
			i--;
		}
		i=1;
		count=1;
		w = (NEWord)word.next;
		while(w!=null&&i<=2){
			String[] lastParts= w.parts;
			for(int j=0;j<lastParts.length;j++)
			{
				sense count:  MyString.normalizeDigitsForFeatureExtraction(lastParts[j]);
				count++;
			}
			w = (NEWord)w.next;
			i++;
		}
  	}
}


// Feature set i
discrete{false, true}% Capitalization(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("Capitalization"))
	{ 
	 	int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next) sense i++ : w.capitalized;
  	}
}

// Feature set ii
discrete{false, true}% WordTypeInformation(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("WordTypeInformation"))
	{ 
	  int i;
	  NEWord w = word, last = word;
	  for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
	  for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

	  for (; w != last; w = (NEWord) w.next, ++i)
	  {
	    boolean allCapitalized = true, allDigits = true, allNonLetters = true;
	
	    for (int j = 0; j < w.form.length(); ++j)
	    {
	      allCapitalized &= Character.isUpperCase(w.form.charAt(j));
	      allDigits &= Character.isDigit(w.form.charAt(j));
	      allNonLetters &= !Character.isLetter(w.form.charAt(j));
	    }

	    sense "c" + i : allCapitalized;
	    sense "d" + i : allDigits;
	    sense "p" + i : allNonLetters;
  	  }
  	}
}

// Feature set iii
discrete% Affixes(NEWord word) <-
{
  	if((!Parameters.tokenizationScheme.equalsIgnoreCase(Parameters.DualTokenizationScheme))
  		&&(!Parameters.tokenizationScheme.equalsIgnoreCase(Parameters.LbjTokenizationScheme)))
  		{
  			System.out.println("Fatal error at Affixes feature extractor: unknown tokenization scheme: " +Parameters.LbjTokenizationScheme);
  			System.exit(0);
  		}

	if(Parameters.featuresToUse.containsKey("Affixes"))
	{ 
  		int N = word.form.length();
	  	for (int i = 3; i <= 4; ++i)
    		if (word.form.length() > i) sense "p|" : word.form.substring(0, i);
  		for (int i = 1; i <= 4; ++i)
    		if (word.form.length() > i) sense "s|" : word.form.substring(N - i);
		
		if(Parameters.tokenizationScheme.equalsIgnoreCase(Parameters.DualTokenizationScheme))	
			for(int i=0;i<word.parts.length;i++)
				sense "part"+i : word.parts[i];

	}
}


discrete NELabel(NEWord word) <- { return word.neLabel; }

real% nonLocalFeatures(NEWord word) <-
{
	//no need to check which features are active here- if 
	//nonlocal features are not used, they will not be generated! 
	String[] feats=word.getAllNonlocalFeatures();
	for(int i=0;i<feats.length;i++)
		sense feats[i]: word.getNonLocFeatCount(feats[i]);
}


// Feature set iv
discrete% PreviousTag1Level1(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("PreviousTag1"))
	{ 
	  	int i;
	  	NEWord w = word;
	  	if(w.previous!=null)
	  	{
			if (NETaggerLevel1.isTraining) 
	    		sense "-1" : ((NEWord)w.previous).neLabel;
	    	else
	    		sense "-1" : ((NEWord)w.previous).neTypeLevel1;
      	}
    }
}

// Feature set iv
discrete% PreviousTag2Level1(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("PreviousTag2"))
	{ 
	  int i;
	  NEWord w = word;
	  if(w.previous!=null)
	  {
		if(((NEWord)w.previous).previous!=null)
	  	{
			if (NETaggerLevel1.isTraining) 
		   		sense "-2" : ((NEWord)((NEWord)w.previous).previous).neLabel;
		   	else
		    	sense "-2" : ((NEWord)((NEWord)w.previous).previous).neTypeLevel1;
	 	}
	  }
  	}
}

real% prevTagsForContextLevel1(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("prevTagsForContext"))
	{
	  	int i,j;
  		NEWord w = word;
		String[] words=new String[3];
		OccurrenceCounter[] count=new OccurrenceCounter[3];
	  	for (i = 0; i <= 2 && w != null; ++i) {
			count[i]=new OccurrenceCounter();
			words[i]=w.form;
			w = (NEWord) w.next;
		}
			
		w=(NEWord)word.previousIgnoreSentenceBoundary;
		for(i=0;i<1000&&w!=null;i++){
			for(j=0;j<words.length;j++){
				if(words[j]!=null&&w.form.equals(words[j])){
						if(NETaggerLevel1.isTraining){
							if(Parameters.prevPredictionsLevel1RandomGenerator.useNoise())
								count[j].addToken(Parameters.prevPredictionsLevel1RandomGenerator.randomLabel());
							else					 
								count[j].addToken(w.neLabel);
						}
						else
			    			count[j].addToken(w.neTypeLevel1);
				}
			}
			w=(NEWord)w.previousIgnoreSentenceBoundary;
		}
	
		for(j=0;j<count.length;j++){
			if(count[j]!=null)	
			{
				String[] all=count[j].getTokens();
				for(i=0;i<all.length;i++)
					sense j+"_"+all[i] : count[j].getCount(all[i])/((double)count[j].totalTokens);	
			}
		}
	}
}

mixed% FeaturesLevel1(NEWord word) <- nonLocalFeatures, GazetteersFeatures, FormParts ,Forms, Capitalization, WordTypeInformation, Affixes, PreviousTag1Level1,PreviousTag2Level1, PreviousTag1Level1&&Forms, prevTagsForContextLevel1, NEShapeTaggerFeatures, BrownClusterPaths,PreviousTag1Level1&&BrownClusterPaths


discrete NETaggerLevel1(NEWord word)  <-
learn NELabel
  using FeaturesLevel1
  with new SparseNetworkLearner(new SparseAveragedPerceptron(.1, 0, 16))
end



//---------------- CLASSIFIER LEVEL 2 -------------------


// Feature set iv
discrete% PreviousTag1Level2(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("PreviousTag1"))
	{ 
	  	int i;
	  	NEWord w = word;
	  	if(w.previous!=null)
	  	{
			if (NETaggerLevel2.isTraining) 
	    		sense "-1" : ((NEWord)w.previous).neLabel;
	    	else
	    		sense "-1" : ((NEWord)w.previous).neTypeLevel2;
      	}
    }
}

// Feature set iv
discrete% PreviousTag2Level2(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("PreviousTag2"))
	{ 
	  int i;
	  NEWord w = word;
	  if(w.previous!=null)
	  {
		if(((NEWord)w.previous).previous!=null)
	  	{
			if (NETaggerLevel2.isTraining) 
		   		sense "-2" : ((NEWord)((NEWord)w.previous).previous).neLabel;
		   	else
		    	sense "-2" : ((NEWord)((NEWord)w.previous).previous).neTypeLevel2;
	 	}
	  }
  	}
}

real% prevTagsForContextLevel2(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("prevTagsForContext"))
	{
	  	int i,j;
  		NEWord w = word;
		String[] words=new String[3];
		OccurrenceCounter[] count=new OccurrenceCounter[3];
	  	for (i = 0; i <= 2 && w != null; ++i) {
			count[i]=new OccurrenceCounter();
			words[i]=w.form;
			w = (NEWord) w.next;
		}
			
		w=(NEWord)word.previousIgnoreSentenceBoundary;
		for(i=0;i<1000&&w!=null;i++){
			for(j=0;j<words.length;j++){
				if(words[j]!=null&&w.form.equals(words[j])){
						if(NETaggerLevel2.isTraining) {
							if(Parameters.prevPredictionsLevel2RandomGenerator.useNoise())
								count[j].addToken(Parameters.prevPredictionsLevel2RandomGenerator.randomLabel());
							else					 
								count[j].addToken(w.neLabel);
						}
						else
			    			count[j].addToken(w.neTypeLevel2);
				}
			}
			w=(NEWord)w.previousIgnoreSentenceBoundary;
		}
	
		for(j=0;j<count.length;j++){
			if(count[j]!=null)	
			{
				String[] all=count[j].getTokens();
				for(i=0;i<all.length;i++)
					sense j+"_"+all[i] : count[j].getCount(all[i])/((double)count[j].totalTokens);	
			}
		}
	}
}


discrete% PatternFeatures(NEWord word) <-
{
  if(Parameters.featuresToUse.containsKey("PatternFeatures"))
  { 
	  OccurrenceCounter typesCounts=new OccurrenceCounter();
	  String[] patterns=word.activePatterns.getTokens();
	  for(int i=0;i<patterns.length;i++){
		  double count=word.activePatterns.getCount(patterns[i]);
			//System.out.println("word:" +word.form+ "pattern: "+patterns[i]+ "  count: " + count);
		  StringTokenizer st=new StringTokenizer(patterns[i]);
		  st.nextToken();
		  typesCounts.addToken(st.nextToken(), count);
	  }	  
	  String[] types=typesCounts.getTokens();
	  double[] weights=new double[types.length];
	  int maxId=0;
	  double sum=0;
	  for(int i=0;i<types.length;i++){
		  weights[i]=typesCounts.getCount(types[i]);
		  if(weights[i]>=weights[maxId])
			maxId=i;
		  sum+=weights[i];
	  }	  
  	for(int i=0;i<types.length;i++)
		  	sense types[i] : weights[i]/sum;
	//if(types.length>0)
	//	sense "majority" : types[maxId];
  }
}



real% level1AggregationFeatures(NEWord word) <-
{
        if(Parameters.featuresToUse.containsKey("PredictionsLevel1")){

                int i=0;
                NEWord w = word, last = (NEWord)word.next;
  
                for (i = 0; i < 2 && last != null; ++i) last = (NEWord) last.next;
                for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;
  
                do 
                {
                                String[] arr=w.mostFrequentLevel1TokenInEntityType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"1"+arr[k] : w.mostFrequentLevel1TokenInEntityType.getCount(arr[k])/w.mostFrequentLevel1TokenInEntityType.totalTokens;
                                arr=w.mostFrequentLevel1SuperEntityType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"2"+arr[k] : w.mostFrequentLevel1SuperEntityType.getCount(arr[k])/w.mostFrequentLevel1SuperEntityType.totalTokens;
                                arr=w.mostFrequentLevel1ExactEntityType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"3"+arr[k] : w.mostFrequentLevel1ExactEntityType.getCount(arr[k])/w.mostFrequentLevel1ExactEntityType.totalTokens;
                                arr=w.mostFrequentLevel1Prediction.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"4"+arr[k] : w.mostFrequentLevel1Prediction.getCount(arr[k])/w.mostFrequentLevel1Prediction.totalTokens;
                                arr=w.mostFrequentLevel1PredictionType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"5"+arr[k] : w.mostFrequentLevel1PredictionType.getCount(arr[k])/w.mostFrequentLevel1PredictionType.totalTokens;
                                arr=w.mostFrequentLevel1NotOutsidePrediction.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"6"+arr[k] : w.mostFrequentLevel1NotOutsidePrediction.getCount(arr[k])/w.mostFrequentLevel1NotOutsidePrediction.totalTokens;
                                arr=w.mostFrequentLevel1NotOutsidePredictionType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"7"+arr[k] : w.mostFrequentLevel1NotOutsidePredictionType.getCount(arr[k])/w.mostFrequentLevel1NotOutsidePredictionType.totalTokens;
                        i++;
                        w = (NEWord) w.next;
                }while(w != last);
        }
}




/*
real% level1AggregationFeatures(NEWord w) <-
{
	if(Parameters.featuresToUse.containsKey("PredictionsLevel1")){
				String[] arr=w.mostFrequentLevel1TokenInEntityType.getTokens();
				for(int k=0;k<arr.length;k++)
					sense "1"+arr[k] : w.mostFrequentLevel1TokenInEntityType.getCount(arr[k])/w.mostFrequentLevel1TokenInEntityType.totalTokens;
				arr=w.mostFrequentLevel1SuperEntityType.getTokens();
				for(int k=0;k<arr.length;k++)
					sense "2"+arr[k] : w.mostFrequentLevel1SuperEntityType.getCount(arr[k])/w.mostFrequentLevel1SuperEntityType.totalTokens;
				arr=w.mostFrequentLevel1ExactEntityType.getTokens();
				for(int k=0;k<arr.length;k++)
					sense "3"+arr[k] : w.mostFrequentLevel1ExactEntityType.getCount(arr[k])/w.mostFrequentLevel1ExactEntityType.totalTokens;
				arr=w.mostFrequentLevel1Prediction.getTokens();
				for(int k=0;k<arr.length;k++)
					sense "4"+arr[k] : w.mostFrequentLevel1Prediction.getCount(arr[k])/w.mostFrequentLevel1Prediction.totalTokens;
				arr=w.mostFrequentLevel1PredictionType.getTokens();
				for(int k=0;k<arr.length;k++)
					sense "5"+arr[k] : w.mostFrequentLevel1PredictionType.getCount(arr[k])/w.mostFrequentLevel1PredictionType.totalTokens;
				arr=w.mostFrequentLevel1NotOutsidePrediction.getTokens();
				for(int k=0;k<arr.length;k++)
					sense "6"+arr[k] : w.mostFrequentLevel1NotOutsidePrediction.getCount(arr[k])/w.mostFrequentLevel1NotOutsidePrediction.totalTokens;
				arr=w.mostFrequentLevel1NotOutsidePredictionType.getTokens();
				for(int k=0;k<arr.length;k++)
					sense "7"+arr[k] : w.mostFrequentLevel1NotOutsidePredictionType.getCount(arr[k])/w.mostFrequentLevel1NotOutsidePredictionType.totalTokens;
	}
}
*/
mixed% FeaturesLevel2(NEWord word) <- level1AggregationFeatures,nonLocalFeatures, GazetteersFeatures, FormParts ,Forms, Capitalization, WordTypeInformation, Affixes, PreviousTag1Level2,PreviousTag2Level2, PreviousTag1Level2&&Forms, prevTagsForContextLevel2, NEShapeTaggerFeatures, BrownClusterPaths,PreviousTag1Level2&&BrownClusterPaths,PatternFeatures



discrete NETaggerLevel2(NEWord word)  <-
learn NELabel
  using FeaturesLevel2
  with new SparseNetworkLearner(new SparseAveragedPerceptron(.1, 0, 20))
end



//------------------------ CONTEXTLESS CLASSIFIER -----------------------

real% charNgrams(NEWord word) <-
{
	OccurrenceCounter grams2=new OccurrenceCounter();
	OccurrenceCounter grams3=new OccurrenceCounter();
	//OccurrenceCounter grams4=new OccurrenceCounter();
	for(int i=0;i<word.form.length()-2;i++)
		grams2.addToken(word.form.substring(i,i+2));
	for(int i=0;i<word.form.length()-3;i++)
		grams3.addToken(word.form.substring(i,i+3));
	//for(int i=0;i<word.form.length()-4;i++)
	//	grams4.addToken(word.form.substring(i,i+4));
	String[] tokens=grams2.getTokens();
	for(int i=0;i<tokens.length;i++)
		sense "grams2"+tokens[i] : grams2.getCount(tokens[i])/grams2.totalTokens;
	tokens=grams3.getTokens();
	for(int i=0;i<tokens.length;i++)
		sense "grams3"+tokens[i] : grams3.getCount(tokens[i])/grams3.totalTokens;
	/*tokens=grams4.getTokens();
	for(int i=0;i<tokens.length;i++)
		sense "grams4"+tokens[i] : grams4.getCount(tokens[i])/grams4.totalTokens;*/		
}

discrete% ShapeFeatures(NEWord word) <-
{
 	int N = word.form.length();
 	for (int i = 3; i <= 4; ++i)
    	if (word.form.length() > i) sense "p|" : word.form.substring(0, i);
  	for (int i = 1; i <= 4; ++i)
    	if (word.form.length() > i) sense "s|" : word.form.substring(N - i);
	
	//	sense "form" : word.form;		
}

discrete length(NEWord word) <- { return word.form.length(); }


discrete NETypeTagger(NEWord word) <-
learn NELabel
  using ShapeFeatures,charNgrams  //, charNgrams&&length, ShapeFeatures&&length 
  with new SparseNetworkLearner(new SparseAveragedPerceptron(.1, 0, 4))
end

real% NEShapeTaggerFeatures(NEWord word) <-
{
	if(Parameters.featuresToUse.containsKey("NEShapeTaggerFeatures")){
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next){
  			if(w.shapePredPer>0)
  		 		sense i+"_Per" : w.shapePredPer;
  			if(w.shapePredOrg>0)
  			 	sense i+"_Org" : w.shapePredOrg;
  			if(w.shapePredLoc>0)
	  		 	sense i+"_Loc" : w.shapePredLoc;
  		 	i++;
  		}
  	}
}
